
## Title

A Mini Project on HIVE which uses a real-life large sized dataset. To see the solved project, run the file Solved Hive Mini Project 2.md or click on the link : 


## Objective

The mini-project is meant for me to apply learnings of the module on Hive on a real-life dataset.
One of the major objectives of this assignment is gaining familiarity with how an analysis works in Hive and how you can gain insights from large dataset.

## Problem Statement

New York City is a thriving metropolis and just like most other cities of similar size, one of the biggest problems its residents face is parking. The classic combination of a huge number of cars and a cramped geography is the exact recipe that leads to a large number of parking tickets.
 
In an attempt to scientifically analyse this phenomenon, the NYC Police Department regularly collects data related to parking tickets. This data is made available by NYC Open Data portal. We will try and perform some analysis on this data.

Download Dataset - https://data.cityofnewyork.us/browse?q=parking+tickets
Note: Consider only the year 2017 for analysis and not the Fiscal year.

Note: both files are csv files. 

1. Create a schema based on the given datasets.
2. Dump the data inside HDFS in the given schema location.
3. List of all agents names. 
4. Find out average rating of each agents.
5. Total working days for each agents.
6. Total query that each agent have taken. 
7. Total Feedback that each agent have received. 
8. Agent names who have average rating between 3.5 to 4. 
9. Agent names who have rating less than 3.5 
10. Agent names who have rating more than 4.5 
11. How many feedbacks agents have received more than 4.5 average
12. Average weekly response time for each agent. 
13. Average weekly resolution time for each agent. 
14. Find the number of chat on which they have received feedbacks. 
15. Total contribution hour for each agent on weekly basis 
16. Perform inner join, left join and right join based on the agent column and after joining the table, export that data into your local system.
17. Perform partitioning on top of the agent column and then on top of that perform bucketing for each partitioning.

## Tech Used

Cloudera Distributed Hadoop --V 5.13.0

HIVE --V 1.1.0

## ðŸš€ About Me
I'm Saheen AHZAN. 

An aspiring Big Data Engineer. Steering to transition my career from retail to Big Data.

GitHub id : saheen619

LinkedIn Profile : https://www.linkedin.com/in/saheenahzan/
